<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QC2GQV1130"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QC2GQV1130');
</script>

  <title>Róbert Csordás</title>
  
  <meta name="author" content="Róbert Csordás">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/IDSIA_logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Róbert Csordás</name>
              </p>
              <p>
                I am a PhD candidate at the <a href="http://www.idsia.ch/">Swiss AI lab IDSIA</a>, working with <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>, where I work on systematic generalization, mainly in context of algorithmic reasoning.
                This drives my research interest in network architectures (Transformers, DNC, graph networks) with inductive biases like information routing (attention, memory) and learning modular structures.
                My goal is to create a system that can learn generally applicable rules instead of pure pattern matching but with minimal hardcoded structure. I consider the lack of systematic generation to be the main obstacle to a more generally applicable artificial intelligence.
            </p>
              <p>
                Prior to starting my PhD I received a master's degree from <a href="https://www.bme.hu/">Budapest University of Technology and Economics</a>, and worked as a research scientist at <a href="https://aimotive.com/">AImotive</a> on developing self driving cars.
              </p>
              <p style="text-align:center">
                <a href="mailto:robert@idsia.ch">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/RobertCsordas/">GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=av1lplwAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/robert_csordas">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/csordas.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/csordas_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                  <img src='images/fastweights.svg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2106.06295">
                <papertitle>Going Beyond Linear Transformers with Recurrent Fast Weight Programmers</papertitle>
              </a>
              <br>
              Kazuki Irie,
              <a href="https://ischlag.github.io/">Imanol Schlag</a>,
              <strong>Róbert Csordás</strong>,
              <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>,
              <br>
        <em>arXiv pre-print, 2021  
              <br>
              <a href="https://arxiv.org/pdf/2106.06295.pdf">pdf</a>
        /
              <a href="https://github.com/IDSIA/recurrent-fwp">code</a>
              <p>
                Inspired by the effectiveness of Fast Weight Programmers in the context of Linear Transformers, in this work we explore the recurrent Fast Weight Programmers (FWPs), which exhibit advantageous properties of both Transformers and RNNs.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/modular.svg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=7uVcpu-gMD">
                <papertitle>Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks</papertitle>
              </a>
              <br>
              <strong>Róbert Csordás</strong>,
              <a href="http://www.sjoerdvansteenkiste.com/">Sjoerd van Steenkiste</a>,
              <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
              <br>
        <em>International Conference on Learning Representations</em>, 2021  
              <br>
              <a href="https://openreview.net/pdf?id=7uVcpu-gMD">pdf</a>
        /
              <a href="https://github.com/RobertCsordas/modules">code</a>
        /
              <a href="https://docs.google.com/presentation/d/1PUMRh1WRmlbWeMmcTWjgPWLAWebwj9IcDYwfcDSY024/edit?usp=sharing">slides</a>
              /
                    <a href="data/modules_poster.pdf">poster</a>
              <br>
              <p>
                This paper presents a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. We contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets using this powerful tool. We demonstrate how common NNs fail to reuse submodules and offer new insights into systematic generalization on language tasks.
              </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/dnc.svg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1904.10278.pdf">
                <papertitle>Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control</papertitle>
              </a>
              <br>
              <strong>Róbert Csordás</strong>,
              <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
              <br>
        <em>International Conference on Learning Representations</em>, 2019
              <br>
        <em>NeurIPS Workshop on Relational Representation Learning</em>, 2018
              <br>
              <a href="https://arxiv.org/pdf/1904.10278.pdf">pdf</a> /
              <a href="https://github.com/robertcsordas/dnc">code</a> /
              <a href="https://docs.google.com/presentation/d/1mn3AwYIzlAL3veoLGV2evKdT5d2e9QCzsORKXYK54i0/edit?usp=sharing">slides</a> /
              <a href="data/poster_dnc.pdf">poster</a>
              <p>
                We propose three improvements for the DNC architecture, which significantly improves its performance on algorithmic reasoning tasks. First, the lack of key-value separation makes the address distribution dependent also on the stored value.  Second, 
                DNC leaves deallocated data in the memory, which results in aliasing. Third, the temporal linkage matrix quickly degrades the sharpness of the address distribution. Our proposed fixes improve the mean error rate on the bAbI question answering dataset by 43%.
              </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patents</heading>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/patent.png' width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://pimg-fpiw.uspto.gov/fdd/53/807/103/0.pdf">
              <papertitle>Method and apparatus for generating a displacement map of an input dataset pair</papertitle>
            </a>
            <br>
            <strong>Róbert Csordás</strong>,
            Ágnes Kis-Benedek,
            Balázs Szalkai
            <br>
      <em>US Patent 10,380,753</em>
            <br>
            <a href="https://pimg-fpiw.uspto.gov/fdd/53/807/103/0.pdf">pdf</a>
            <p>
              We propose a fast and accurate method for generating displacement maps from stereo image pairs using neural networks. This enables 
              more robust depth prediction compared to standard methods.
            </p>
          </td>
        </tr>

        </tbody>
      </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Talks</heading>
      <ul>
          <li> In July 2021 I gave a remote talk at the Center for Machine Learning and Neuroscience at the Baylor college of Medicine in Houston on the paper <a href="https://openreview.net/pdf?id=7uVcpu-gMD">Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks</a>.</li>
      </ul>
          </td>
        </tr>
      </tbody>
      </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template credits to <a href="https://jonbarron.info">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody>
        </table>
      
      </table>
      </td>
    </tr>
  </table>
</body>

</html>
